{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b86e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树结构:\n",
      "petal length (cm) < 4.45:\n",
      "Left:\n",
      " petal length (cm) < 3.3:\n",
      " Left:\n",
      "  sepal width (cm) < 3.5:\n",
      "  Left:\n",
      "   Leaf: setosa\n",
      "  Right:\n",
      "   Leaf: setosa\n",
      " Right:\n",
      "  Leaf: versicolor\n",
      "Right:\n",
      " petal length (cm) < 5.5:\n",
      " Left:\n",
      "  petal width (cm) < 1.75:\n",
      "  Left:\n",
      "   Leaf: versicolor\n",
      "  Right:\n",
      "   Leaf: virginica\n",
      " Right:\n",
      "  Leaf: virginica\n",
      "测试集准确率: 0.9667\n"
     ]
    }
   ],
   "source": [
    "#导入必要的库\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import  Counter\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.datasets import  load_iris\n",
    "\n",
    "#实现计算熵的函数\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"计算数据集的熵\"\"\"\n",
    "    if len(y) ==0:\n",
    "        return 0\n",
    "    #统计每个类别的数量\n",
    "    counter=Counter(y)  \n",
    "    \n",
    "    \"\"\"计算熵\"\"\"\n",
    "    entropy=0\n",
    "    for count in counter.values():\n",
    "        probablity=count/len(y)\n",
    "        entropy-=probablity*np.log2(probablity)\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\"\"\"实现计算信息增益的函数\"\"\"\n",
    "def calculate_information_gain(X,y,feature_index):\n",
    "    \"\"\"计算按照feature_index划分数据集后的信息增益\"\"\"\n",
    "    \n",
    "    #计算父节点的熵\n",
    "    parent_entropy=calculate_entropy(y)\n",
    "    \n",
    "    #获取该特征的所有唯一值\n",
    "    feature_values=np.unique(X[:,feature_index])\n",
    "    \n",
    "    #计算按照该特征划分后的熵\n",
    "    children_entropy=0\n",
    "    for value in feature_values:\n",
    "        #获取该特征值对应的索引\n",
    "        child_indexs=np.where(X[:,feature_index]==value)[0]\n",
    "        \n",
    "        #计算子节点的权重\n",
    "        weight=len(child_indexs)/len(y)\n",
    "        #计算子节点的熵\n",
    "        child_entropy=calculate_entropy(y[child_indexs])\n",
    "        #累加子节点的熵\n",
    "        children_entropy+=weight*child_entropy\n",
    "        \n",
    "    #计算信息增益\n",
    "    information_gain=parent_entropy-children_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "\"\"\"实现决策树的节点类\"\"\"\n",
    "class DecisionNode:  \n",
    "    \"\"\"决策树的节点类\"\"\"\n",
    "    def __init__(self,feature_index=None,threshold=None,left=None,right=None,value=None):\n",
    "        \"\"\"初始化节点\"\"\"\n",
    "        self.feature_index=feature_index #特征索引\n",
    "        self.threshold=threshold #特征阈值\n",
    "        self.left=left  #左子树\n",
    "        self.right=right #右子树\n",
    "        self.value=value #节点的预测值 \n",
    "        \n",
    "\"\"\"实现决策树的树类\"\"\"\n",
    "class DecisionTree: \n",
    "    \"\"\"自己实现的决策树\"\"\"\n",
    "    def __init__(self,max_depth=None):\n",
    "        self.max_depth=max_depth  #最大深度\n",
    "        self.root=None\n",
    "    def _best_spilt(self,X,y):\n",
    "        \"\"\"寻找最佳的划分特征以及划分的阈值\"\"\"\n",
    "        m,n=X.shape\n",
    "        if m<=1 :\n",
    "            return None,None\n",
    "        \n",
    "        #计算父节点的熵 \n",
    "        parent_entropy=calculate_entropy(y)\n",
    "        \n",
    "        #计算最佳信息增益和对应的特征 \n",
    "        best_gain=0 \n",
    "        best_feature_index=None\n",
    "        \n",
    "        #遍历所有特征\n",
    "        for feature_index in range(n):\n",
    "            #计算该信息的信息增益 \n",
    "            info_gain=calculate_information_gain(X,y,feature_index)\n",
    "            \n",
    "            #更新最佳信息增益和对应的特征\n",
    "            if  info_gain>best_gain:\n",
    "                best_gain=info_gain\n",
    "                best_feature_index=feature_index\n",
    "        return best_feature_index, best_gain        \n",
    "        \n",
    "    def _build_tree(self,X,y,depth=0):\n",
    "        \"\"\"递归构造决策树\"\"\"\n",
    "        \n",
    "        m,n=X.shape\n",
    "        \n",
    "        #检查停止条件\n",
    "        if (self.max_depth is not None and depth>=self.max_depth) or len(set(y))==1:\n",
    "            #如果达到最大深度或者所有样本属于同一类别，则创建叶节点\n",
    "            leaf_value=Counter(y).most_common(1)[0][0]\n",
    "            return DecisionNode(value=leaf_value)\n",
    "        #寻找最佳划分特征和信息增益\n",
    "        best_feature_index,best_gain=self._best_spilt(X,y)\n",
    "        \n",
    "        #如果没找到好的划分特征 \n",
    "        if best_feature_index is None:\n",
    "            leaf_value=Counter(y).most_common(1)[0][0] \n",
    "            return DecisionNode(value=leaf_value)\n",
    "        \n",
    "        #如果找到了最优的划分特征 则按照该划分的特征进行分类工作 \n",
    "        feature_values=np.unique(X[:,best_feature_index])\n",
    "        \n",
    "        #如果特征只有一个值，则创建叶节点 \n",
    "        if len(feature_values)==1:\n",
    "            leaf_value=Counter(y).most_common(1)[0][0]\n",
    "            return DecisionNode(value=leaf_value)\n",
    "        \n",
    "        #对于标称型特征，取中位数作为阈值\n",
    "        threshold=np.median(feature_values)\n",
    "        #划分数据集\n",
    "        left_indexs=np.where(X[:,best_feature_index]<threshold)[0]\n",
    "        right_indexs=np.where(X[:,best_feature_index]>=threshold)[0]\n",
    "        \n",
    "        #递归构建左子树和右子树\n",
    "        left_subtree=self._build_tree(X[left_indexs],y[left_indexs],depth+1)\n",
    "        right_subtree=self._build_tree(X[right_indexs],y[right_indexs],depth+1)\n",
    "        \n",
    "        #返回决策节点\n",
    "        return DecisionNode(feature_index=best_feature_index, threshold=threshold, left=left_subtree, right=right_subtree)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"训练决策树模型\"\"\"\n",
    "        self.root=self._build_tree(X,y)\n",
    "        return self    \n",
    "    \n",
    "    def predict_sample(self,x,node):\n",
    "        \"\"\"预测单个样本\"\"\"\n",
    "        #如果是叶节点，则返回预测值\n",
    "        if node.value is not None:\n",
    "            return  node.value  \n",
    "        \n",
    "        #如果不是叶节点，则根据特征索引和阈值进行分类 走左子树还是右子树 \n",
    "        if x[node.feature_index] <node.threshold:\n",
    "            return self.predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.predict_sample(x,node.right) \n",
    "        \n",
    "    def predict(self,X):\n",
    "        \"\"\"预测多个样本\"\"\"\n",
    "        return  np.array([self.predict_sample(x,self.root) for x in X])\n",
    "    \n",
    "def visualize_tree(node,feature_names=None,class_names=None,depth=0):\n",
    "    \"\"\"可视化决策树\"\"\"\n",
    "    indent=\" \"* depth\n",
    "    \n",
    "    if node.value is not None:\n",
    "        class_idx=node.value\n",
    "        class_name=class_names[class_idx] if class_names is not None else str(class_idx)\n",
    "        print(f\"{indent}Leaf: {class_name}\")\n",
    "        return \n",
    "    feature_name=feature_names[node.feature_index]if feature_names else f\"Feature {node.feature_index}\"\n",
    "    print(f\"{indent}{feature_name} < {node.threshold}:\")\n",
    "    \n",
    "    print(f\"{indent}Left:\")\n",
    "    visualize_tree(node.left, feature_names, class_names, depth + 1)\n",
    "    print(f\"{indent}Right:\")\n",
    "    visualize_tree(node.right, feature_names, class_names, depth + 1)\n",
    "    \n",
    "#加载数据集进行测试 \n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 分割训练集和测试集\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[indices[:train_size]], X[indices[train_size:]]\n",
    "y_train, y_test = y[indices[:train_size]], y[indices[train_size:]]\n",
    "\n",
    "# 训练决策树模型\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# 可视化决策树\n",
    "print(\"决策树结构:\")\n",
    "visualize_tree(tree.root, iris.feature_names, iris.target_names)\n",
    "\n",
    "# 在测试集上评估模型\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(f\"测试集准确率: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009818b6",
   "metadata": {},
   "source": [
    "# 决策树算法实现与分析实验报告\n",
    "\n",
    "## 1. 实验概述\n",
    "\n",
    "本实验实现了一个基于信息增益的决策树分类算法，并在经典的鸢尾花数据集上进行了测试。实验包含了决策树的核心算法实现、模型训练、预测和性能评估等完整流程。\n",
    "\n",
    "## 2. 理论基础\n",
    "\n",
    "### 2.1 信息熵（Information Entropy）\n",
    "\n",
    "信息熵是衡量数据集纯度的重要指标，定义为：\n",
    "\n",
    "$$H(S) = -\\sum_{i=1}^{k} p_i \\log_2(p_i)$$\n",
    "\n",
    "其中：\n",
    "- $S$ 是数据集\n",
    "- $k$ 是类别数量\n",
    "- $p_i$ 是第 $i$ 类样本在数据集中的比例\n",
    "\n",
    "熵值越小，数据集的纯度越高；熵值为0时，数据集完全纯净（所有样本属于同一类）。\n",
    "\n",
    "### 2.2 信息增益（Information Gain）\n",
    "\n",
    "信息增益衡量了按某个特征划分数据集后纯度的提升程度：\n",
    "\n",
    "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
    "\n",
    "其中：\n",
    "- $A$ 是待评估的特征\n",
    "- $Values(A)$ 是特征 $A$ 的所有可能取值\n",
    "- $S_v$ 是特征 $A$ 取值为 $v$ 的样本子集\n",
    "\n",
    "信息增益越大，说明该特征对分类的贡献越大。\n",
    "\n",
    "### 2.3 决策树构建原理\n",
    "\n",
    "决策树采用递归的方式构建：\n",
    "1. 选择信息增益最大的特征作为当前节点的划分标准\n",
    "2. 根据该特征的取值将数据集分割成子集\n",
    "3. 对每个子集递归执行上述过程\n",
    "4. 满足停止条件时创建叶节点\n",
    "\n",
    "## 3. 算法实现分析\n",
    "\n",
    "### 3.1 核心函数实现\n",
    "\n",
    "**熵计算函数**：\n",
    "- 使用 `Counter` 统计各类别频次\n",
    "- 计算每个类别的概率\n",
    "- 应用信息熵公式进行计算\n",
    "\n",
    "**信息增益计算**：\n",
    "- 计算父节点熵值\n",
    "- 遍历特征的所有唯一值\n",
    "- 计算加权平均的子节点熵值\n",
    "- 返回信息增益\n",
    "\n",
    "### 3.2 决策树结构\n",
    "\n",
    "**节点类设计**：\n",
    "- `feature_index`：划分特征的索引\n",
    "- `threshold`：划分阈值\n",
    "- `left/right`：左右子树\n",
    "- `value`：叶节点的预测值\n",
    "\n",
    "**树类实现**：\n",
    "- `_best_split()`：寻找最佳划分特征\n",
    "- `_build_tree()`：递归构建决策树\n",
    "- `predict()`：预测新样本\n",
    "\n",
    "## 4. 实验结果分析\n",
    "\n",
    "### 4.1 数据集概况\n",
    "\n",
    "- **数据集**：鸢尾花数据集（Iris Dataset）\n",
    "- **样本总数**：150个样本\n",
    "- **特征维度**：4维（花萼长度、花萼宽度、花瓣长度、花瓣宽度）\n",
    "- **类别数量**：3类（山鸢尾、变色鸢尾、维吉尼亚鸢尾）\n",
    "- **训练集**：120个样本（80%）\n",
    "- **测试集**：30个样本（20%）\n",
    "\n",
    "### 4.2 模型性能\n",
    "\n",
    "**测试集准确率**：96.67%\n",
    "\n",
    "这个结果表明：\n",
    "- 决策树在鸢尾花数据集上表现优异\n",
    "- 仅有1个样本分类错误（30个测试样本中）\n",
    "- 算法实现正确且有效\n",
    "\n",
    "### 4.3 决策树结构分析\n",
    "\n",
    "通过可视化输出可以观察到：\n",
    "- 树的最大深度被限制为3层，防止过拟合\n",
    "- 每个内部节点选择信息增益最大的特征进行划分\n",
    "- 叶节点给出最终的分类结果\n",
    "\n",
    "## 5. 数学原理深度分析\n",
    "\n",
    "### 5.1 为什么选择信息增益？\n",
    "\n",
    "信息增益基于信息论，具有以下优势：\n",
    "- **理论基础扎实**：基于香农信息论\n",
    "- **直观易懂**：信息增益大意味着分类效果好\n",
    "- **计算简单**：公式简洁，计算复杂度低\n",
    "\n",
    "### 5.2 阈值选择策略\n",
    "\n",
    "对于连续特征，本实现选择中位数作为划分阈值：\n",
    "- **优点**：简单有效，对异常值不敏感\n",
    "- **改进空间**：可以尝试所有可能的划分点，选择信息增益最大的\n",
    "\n",
    "### 5.3 停止条件设计\n",
    "\n",
    "算法设置了多个停止条件：\n",
    "- 达到最大深度限制\n",
    "- 所有样本属于同一类别\n",
    "- 无法找到有效的划分特征\n",
    "- 特征只有单一取值\n",
    "\n",
    "## 6. 算法优缺点分析\n",
    "\n",
    "### 6.1 优点\n",
    "\n",
    "- **可解释性强**：决策过程清晰直观\n",
    "- **无需数据预处理**：能处理数值型和类别型特征\n",
    "- **计算效率高**：训练和预测速度快\n",
    "- **处理缺失值能力强**：可以设计相应策略\n",
    "\n",
    "### 6.2 缺点\n",
    "\n",
    "- **容易过拟合**：特别是在树深度较大时\n",
    "- **对噪声敏感**：异常值可能影响划分质量\n",
    "- **偏向多值特征**：信息增益偏好取值较多的特征\n",
    "- **不稳定性**：训练数据的微小变化可能导致完全不同的树\n",
    "\n",
    "## 7. 总结与展望\n",
    "\n",
    "本实验成功实现了基于信息增益的决策树算法，在鸢尾花数据集上取得了96.67%的高准确率。实验验证了决策树算法的有效性，同时也展示了其良好的可解释性。\n",
    "\n",
    "**改进方向**：\n",
    "- 实现信息增益率（C4.5算法）来处理多值特征偏向问题\n",
    "- 加入剪枝机制来减少过拟合\n",
    "- 支持连续特征的最优划分点搜索\n",
    "- 实现随机森林等集成方法提升性能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
